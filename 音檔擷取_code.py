# -*- coding: utf-8 -*-
"""音檔擷取_code.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QppBg-oh8wq06HNdBClng2oxrMgEA3lF
"""

pip install python_speech_features

import pandas as pd 
import numpy as np
import numpy
import matplotlib.pyplot as plt
import seaborn as sns
import os
import re
import torch
import torch.nn as nn
import librosa
import soundfile as sf
import wave
import python_speech_features
import tqdm as notebook_tqdm
from torch import nn, matmul
from torch.nn import functional as F
from torch.nn.functional import softmax
from torch.utils.data import Dataset
from python_speech_features import mfcc

"""#匯入資料夾"""

from google.colab import drive
drive.mount('/content/drive')

lab_Path='/content/drive/MyDrive/蘇筱凌/lab_dataset/lab_data_v3'
tele_Path='/content/drive/MyDrive/蘇筱凌/customer_dataset/tele_data_v2'

"""#實驗室和電話訪談資料集"""

lab_data=[]
for wav in os.listdir(lab_Path):
    speech=wav.partition(".wav")[0]
    re.findall(r'[0-9]+|[A-Z]+',speech)
    if speech[0]=='S':
        lab_data.append(('S',lab_Path+'/'+wav))
    elif speech[0]=='N':
        lab_data.append(('N',lab_Path+'/'+wav))
    elif speech[0]=='U':
        lab_data.append(('U',lab_Path+'/'+wav))
    else:
        lab_data.append(('unknown',lab_Path+'/'+wav))
lab_data_df=pd.DataFrame.from_dict(lab_data)
lab_data_df.rename(columns={0:'label',1:'wav_path',2:'wav_name'},inplace=True)
lab_data_df

lab_data_df['wav_name'] = lab_data_df.wav_path.str.split(lab_Path+"/", expand = True).loc[:, 1]

tele_data=[]
for wav in os.listdir(tele_Path):
    speech=wav.partition(".wav")[0]
    re.findall(r'[0-9]+|[A-Z]+',speech)
    if speech[0]=='N':
        tele_data.append(('N',tele_Path+'/'+wav))
    elif speech[0]=='Y':
        tele_data.append(('Y',tele_Path+'/'+wav))
    else:
        tele_data.append(('unknown',tele_Path+'/'+wav))
tele_data_df=pd.DataFrame.from_dict(tele_data)
tele_data_df.rename(columns={0:'label',1:'wav_path',2:'wav_name'},inplace=True)
tele_data_df.head()

tele_data_df['wav_name'] = tele_data_df.wav_path.str.split(tele_Path+"/", expand = True).loc[:, 1]

lab_data_df

tele_data_df

"""#音檔info(頻譜圖)"""

def plot_spectrogram(row):
    file_path = row['wav_path']
    wav_name = row['wav_name']
    y, sr = librosa.load(file_path)
    D = librosa.amplitude_to_db(librosa.stft(y), ref=np.max)
    plt.figure(figsize=(6, 4))
    librosa.display.specshow(D, y_axis='linear', x_axis='time')
    plt.colorbar(format='%+2.0f dB')
    
    title = f'{wav_name}\nLinear-frequency power spectrogram'
    plt.title(title, fontsize=10)
    
    plt.show()

lab_data_df.apply(plot_spectrogram, axis=1)

"""#音檔resample頻率儲存"""

def resample_files(input_dir, output_dir, target_sr):
    # 檢查輸出目錄是否存在，如果不存在就創建它
    os.makedirs(output_dir, exist_ok=True)

    # 遍歷輸入目錄中的所有檔案
    for file_name in os.listdir(input_dir):
        # 只處理 WAV 檔案
        if file_name.endswith('.wav'):
            # 構造輸入檔案路徑和輸出檔案路徑
            input_path = os.path.join(input_dir, file_name)
            output_path = os.path.join(output_dir, file_name)

            # 讀取音訊檔案
            y, sr = librosa.load(input_path, sr=None, offset=0, duration=400)

            # 將音訊檔案重採樣
            y_resampled = librosa.resample(y, orig_sr=sr, target_sr=target_sr)

            # 保存重採樣後的音訊檔案
            sf.write(output_path, y_resampled, target_sr)

resample_files('/content/drive/MyDrive/蘇筱凌/lab_dataset/lab_data_v1', '/content/drive/MyDrive/蘇筱凌/lab_dataset/resample', 9000)

resample_files('/content/drive/MyDrive/蘇筱凌/customer_dataset/tele_data_v2', '/content/drive/MyDrive/蘇筱凌/customer_dataset/resample_tele_data', 8000)

"""#轉MFCC特徵"""

def mfcc_feature(audio, samplerate=44100):
    mfcc_feature = python_speech_features.mfcc(signal=audio, 
                          samplerate=samplerate, 
                          winlen=0.025,
                          winstep=0.01, #overlap 
                          numcep=13,
                          nfilt=40, #30
                          nfft=2048,
                          preemph=0.97,
                          winfunc=numpy.hamming,
                          appendEnergy=False)
    return mfcc_feature

def extract_features(signal,samplerate):
    result = np.array([])
    result = mfcc_feature(signal,samplerate)
    print('result', result.shape)
    return result

def lab_get_features(path, sr=44100, duration=0.6, offset=0.0):
    #signal,samplerate=sf.read(path)
    signal,samplerate=librosa.load(path, sr=sr, duration=duration, offset=offset)
    mfcc_feat = extract_features(signal,samplerate)
    audio=np.array(mfcc_feat)
    return audio

def tele_get_features(path, sr=44100, duration=400.0 ,offset=0.0):
    #signal,samplerate=sf.read(path)
    signal,samplerate=librosa.load(path, sr=sr, duration=duration, offset=offset)
    mfcc_feat = extract_features(signal,samplerate)
    audio=np.array(mfcc_feat)
    return audio

def convert_lab_features():
    P,Q=[],[]
    for path,name,index in zip(lab_data_df.wav_path,lab_data_df.wav_name,range(lab_data_df.wav_path.shape[0])):
        print("第",index+1,"個音檔",name)
        features=lab_get_features(path)
        pad_features = np.pad(features, [(0, 59-len(features)), (0,0)], 'constant', constant_values = (0))
        for i in pad_features:
            P.append(i)
            Q.append(name)
    print('Done')
    P_array = np.array(P)
    P_array.resize((lab_data_df.wav_path.shape[0], 59, 13))
    return P_array

def convert_tele_features():
    P,Q=[],[]
    for path,name,index in zip(tele_data_df.wav_path,tele_data_df.wav_name,range(tele_data_df.wav_path.shape[0])): #tele_data_df.wav_path.shape[0]
        print("第",index+1,"個音檔",name)
        features=tele_get_features(path)
        pad_features = np.pad(features, [(0, 40120-len(features)), (0,0)], 'constant', constant_values = (0))
        for i in pad_features:
            P.append(i)
            Q.append(name)     
    print('Done')
    P_array = np.array(P)
    P_array.resize((tele_data_df.wav_path.shape[0], 40120, 13)) #400秒:59*680=40120 380秒:59*650=38350
    return P_array

test_x = convert_lab_features()

test_y = convert_tele_features()

"""#讀轉好的test_y
test_y_cut20_nfilt30:電訪資料、nfilt設30、-20秒
"""

import pickle

with open('/content/drive/MyDrive/蘇筱凌/code/test_y_nfilt40.pkl', 'wb') as f:
    pickle.dump(test_y, f)

data_input = open('/content/drive/MyDrive/蘇筱凌/code/test_y_nfilt40.pkl','rb')
test_y = pickle.load(data_input)
data_input.close()

"""#滑動窗格
返回一組特徵向量
"""

def sliding_window(seq, window_size, step_size):
    for i in range(0, len(seq) - window_size + 1, step_size):
        yield seq[i:i+window_size]

"""#Model
Cross attention、計算cos simiraity

###F.cosine_similarity(k, q, dim=0)
"""

class CrossAttention(nn.Module):
    def __init__(self, dim, num_heads=None, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):
        super(CrossAttention, self).__init__()
        self.num_heads = num_heads
        #head_dim = dim // num_heads
        #NOTE scale factor was wrong in my original version, can set manually to be compat with prev weights
        self.scale = dim ** -0.5 #head_dim ** -0.5     
        
        self.wk = nn.Linear(dim, dim, bias=qkv_bias)
        self.wq = nn.Linear(dim, dim, bias=qkv_bias)
        self.wv = nn.Linear(dim, dim, bias=qkv_bias)
        self.softmax = nn.Softmax(dim=2)
        self.attn_drop = nn.Dropout(attn_drop)

    def forward(self, key, query):
        key = torch.from_numpy(key)
        key = key.to(torch.float32)
        query = torch.from_numpy(query)
        query = query.to(torch.float32)
        #k = self.wk(key)
        #q = self.wq(query)
        idx_list = []
        cos_scores = []
        wavs = []
        for i, k in enumerate(key): #lab音檔
          k = k.flatten()
        #Cos Similarity
          for idx, q in enumerate(sliding_window(query, 59, 2)): #電訪音檔，一次59、shift59
            q = q.flatten()
            cos_sim = F.cosine_similarity(k, q, dim=0)
            idx_list.append(idx)
            cos_scores.append(cos_sim.item()) #存所有相似度分數
            wavs.append(i) #對應到的lab音檔
        # 取出前五個元素
        top_five_scores_sorted = []
        result_list = []
        used_idx_set = set()
        for idx, cos_score, wav in sorted(zip(idx_list, cos_scores, wavs), key=lambda x: x[1], reverse=True):
          if idx < 1000:
            continue
          if idx in used_idx_set:
            continue
          top_five_scores_sorted.append((idx, cos_score, wav))
          used_idx_set.add(idx)
          if len(top_five_scores_sorted) == 6:
            break        
          result_list.append((idx, cos_score, wav))
          print(f"Index: {idx}, Score: {cos_score}, Wav: {wav}")
  
        # top_five_scores_sorted = sorted(zip(idx_list, cos_scores, wavs), key=lambda x: x[1], reverse=True)[:10]
        # result_list = []
        # for idx, score, wavs in top_five_scores_sorted:
        #   result_list.append((idx, score, wavs))
        #   print(f"Index: {idx}, Score: {score}, Wav: {wavs}")
        
        return result_list

"""###dot product torch.matmul"""

class CrossAttention(nn.Module):
    def __init__(self, dim, num_heads=None, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):
        super(CrossAttention, self).__init__()
        self.num_heads = num_heads
        self.scale = dim ** -0.5 #head_dim ** -0.5     
        
        self.wk = nn.Linear(dim, dim, bias=qkv_bias)
        self.wq = nn.Linear(dim, dim, bias=qkv_bias)

    def forward(self, key, query):
        key = torch.from_numpy(key)
        key = key.to(torch.float32)
        query = torch.from_numpy(query)
        query = query.to(torch.float32)
        
        # key = self.wk(key)
        # query = self.wq(query)
        idx_list = []
        cos_scores = []
        wavs = []
        #dot product
        for i, k in enumerate(key): #lab音檔
          k = k.flatten()
          for idx, q in enumerate(sliding_window(query, 59, 59)): #電訪音檔，一次59、shift59音檔
            q = q.flatten()
            cos_sim = torch.matmul(k, q)/ self.scale
            idx_list.append(idx)
            cos_scores.append(cos_sim.item()) #存所有相似度分數
            wavs.append(i) #對應到的lab音檔
        # 取出前五個元素
        top_five_scores_sorted = []
        result_list = []
        used_idx_set = set()
        for idx, cos_score, wav in sorted(zip(idx_list, cos_scores, wavs), key=lambda x: x[1], reverse=True):
          if idx < 35:
            continue
          if idx in used_idx_set:
            continue
          if cos_score == 0:
            continue
          top_five_scores_sorted.append((idx, cos_score, wav))
          used_idx_set.add(idx)
          if len(top_five_scores_sorted) == 6:
            break        
          result_list.append((idx, cos_score, wav))
          print(f"Index: {idx}, Score: {cos_score}, Wav: {wav}")
  
        # top_five_scores_sorted = sorted(zip(idx_list, cos_scores, wavs), key=lambda x: x[1], reverse=True)[:10]
        # result_list = []
        # for idx, score, wavs in top_five_scores_sorted:
        #   result_list.append((idx, score, wavs))
        #   print(f"Index: {idx}, Score: {score}, Wav: {wavs}")
        
        return result_list

"""###vote"""

from collections import Counter

class CrossAttention(nn.Module):
    def __init__(self, dim, num_heads=None, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):
        super(CrossAttention, self).__init__()
        self.num_heads = num_heads
        #head_dim = dim // num_heads
        #NOTE scale factor was wrong in my original version, can set manually to be compat with prev weights
        self.scale = dim ** -0.5 #head_dim ** -0.5     
        
        self.wk = nn.Linear(dim, dim, bias=qkv_bias)
        self.wq = nn.Linear(dim, dim, bias=qkv_bias)
        self.wv = nn.Linear(dim, dim, bias=qkv_bias)
        self.softmax = nn.Softmax(dim=2)
        self.attn_drop = nn.Dropout(attn_drop)

    def forward(self, key, query):
        key = torch.from_numpy(key)
        key = key.to(torch.float32)
        query = torch.from_numpy(query)
        query = query.to(torch.float32)
        #k = self.wk(key)
        #q = self.wq(query)
        idx_list = []
        cos_scores = []
        wavs = []
        for i, k in enumerate(key): #實驗室資料140筆(24,13)
          k = k.flatten()
        #Cos Similarity        
          for idx, q in enumerate(sliding_window(query, 59, 10)): #電訪音檔，一次59、shift59
            q = q.flatten()
            cos_sim = F.cosine_similarity(k, q, dim=0)
            cos_scores.append(cos_sim.item()) #存所有相似度分數
            
        cos_scores = np.reshape(cos_scores, (28, 2008)) #reshap成(140,2008)
        cos_scores = cos_scores.tolist()

        for scores in cos_scores:
          max_val = max(scores)
          max_similarity.append(max_val)
          max_idx = scores.index(max_val)
          max_index.append(max_idx)
        max_scores = list(zip(max_index, max_similarity))
        print(max_index)
          
        counted_values = Counter(max_index)
        most_index, occurrences = counted_values.most_common(1)[0]
        print(most_index, occurrences)
        return most_index

"""#輸入資料"""

result=[]
for index,element in enumerate(test_y):
  print("電訪的第",index+1,"音檔")
  attn = CrossAttention(13)
  result.append(attn(test_x, element))

with open('/content/drive/MyDrive/蘇筱凌/code/result_40_step2.pkl', 'wb') as f:
  pickle.dump(result, f)

"""#讀取結果，轉csv"""

data_input = open('/content/drive/MyDrive/蘇筱凌/code/result_40_step20.pkl','rb')
result = pickle.load(data_input)
data_input.close()

#取array加上空格
new_result=[]
for row in result:
  for i in row:
    new_result.append(i)  
  new_result.append('')   
  new_result.append('')   
  new_result.append('')   
  new_result.append('')   
  new_result.append('')   
  new_result.append('')

new_result = pd.DataFrame(new_result)
new_result = new_result.rename(columns={0: 'idx', 1:'cos_scores', 2:'wav'})
new_result

path = '/content/drive/MyDrive/蘇筱凌/code'

#轉csv
new_result.to_csv(path+'/result_40_step20.csv',index=False)